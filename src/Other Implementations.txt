** Toy Paxos (Python)
[http://the-paper-trail.org/blog/?p=190]

    - Leader and Acceptor are implemented
    - PaxosLeader listens for proposals from external clients and 
    runs the protocol with whichever PaxosAcceptors are currently
    correct/connected

    - If a leader fails, another leader will take over once it realises. 
    If clients fail, the protcol will still run until more than half have 
    failed.

    - Heartbeats are used to do weak Leader Election.

    - GarbageCollection: If a proposal is accepted it is removed.

    - Leader & Acceptor run following a state machine.

** Lib Paxos (C)
[http://libpaxos.sourceforge.net/]
    - 5 Different Implementations:
      		  -- RingPaxos: a high-throughput implementation based on IP-Mmulticast
		  -- libPaxos2 : libPaxos with leader election and various protocol optimizations
		  -- libPaxos   : basic implementation
		  -- fastPaxos : fast paxos algorithm
		  -- erlangPxs : simulator written in Erlang

      --- fastPaxos
      	  - disk writes to keep state

** pyPaxos (Python)
[http://evanjones.ca/model-checking-paxos.html]
    - basic paxos algorithm
    - algorithm tested by model checking
    - brute force testing using orderings
      	    - employing different partial orders

** Chubby

    - coarse-grained synchronization
    - electing a leader in a group
    - paxos maintains safety without timing assumptions, 
    but clocks must be introduced to ensure liveness
    - electing a master in a file server: to elect a master 
    which then writes to an existing file server requires 
    adding just two statements and one RPC parameter 
    to an existing system: One would acquire a lock to 
    become master, pass an additional inte- ger (the 
    lock acquisition count) with the write RPC, and add 
    an if-statement to the file server to reject the write 
    if the acquisition count is lower than the current 
    value (to guard against delayed packets)
    - a server, and a library that client applications link
    communicate via RPC
    - chubby cell consists of replicas
    - replicas use a distributed consensus protocol to
    elect a master
    	  !!- the master must obtain votes from a majority
	  of the replicas, plus promises that those replicas
	  will not elect a different master for an interval of
	  a few seconds known as the *master lease*
	  -- the master lease is periodically renewed by the
	  replicas provided the master continues to win a
	  majority of the vote
    -  replicas maintain copies of a simple database, but
    only the master initiates reads and writes of this 
    database. All other replicas simply copy updates from
    the master, sent using the consensus protocol
    !! clients find the master by sending master location 
    requests to the replicas listed in the DNS, replicas 
    respond to such requests by returning the identity 
    of the master
    !! write requests are propagated via the consensus 
    protocol to all replicas; such requests are acknowledged 
    when the write has reached a majority of the replicas in 
    the cell
    !! read requests are satisfied by the master alone; this is 
    safe provided the master lease has not expired, as no 
    other master can possibly exist
    **** Master Fail ****
    if a *master fails*, the other replicas run the election 
    protocol when their master leases expire; a new master 
    will typically be elected in a few seconds
    **** Replica Fail ****
    if a replica fails and does not recover for a few hours, a 
    simple replacement system selects a fresh machine from 
    a free pool and starts the lock server binary on it. It then 
    updates the DNS tables, replacing the IP address of the 
    failed replica with that of the new one. The current master
    polls the DNS periodically and eventually notices the change. 
    It then updates the list of the cell’s members in the cell’s 
    database; this list is kept consistent across all the members
    via the normal replication protocol. In the meantime, the 
    new replica obtains a recent copy of the database from a 
    combination of backups stored on file servers and updates
    from active replicas. Once the new replica has processed a
    request that the current master is waiting to commit,
    the replica is permitted to vote in the elections for new master.
     
